---
layout: post
title: Some Puns and a theory of Humour
description: Knowledge based AI note on Puns.
img:  KBAI-overview.png
date: 2021-04-13 00:00:00 +0300
img: nlp-brain-wordcloud.jpg
fig-caption: Knowledge Based AI - Theory of Humor
tags: [modelling, artificial intelligence, AI, KBAI, puns, humor] 
---

I was kidding about the theory of humour. :joy: 

[Puns](https://en.wikipedia.org/wiki/Pun) may be funny :laughing: due to a [double entendre](https://en.wikipedia.org/wiki/Double_entendre)  :confused:  caused by ambiguity :thinking:  in their wording. The double meaning is an outcome is due to some ambiguity. They are certainly more memorable than many examples from Grammar text books. Being able to readily recall great examples is a hallmark of excellence, particularly when discussing linguistics. :zzz:
<!--more-->

**Contents**
* This will become a table of contents (this text will be scrapped).
{:toc}


# On the use of puns


>I was wondering why the ball was getting bigger and then it *hit* me

@@
hit
\begin{cases}
WS_1 :\text{struck} \newline
WS_2 :\text{understood}
\end{cases}
@@

>Two men walk into a *bar*, the third one ducks

In this one has bar has one senese in the first phrase and anther in the second.

@@
bar
\begin{cases}
WS_1: \text{pub} \newline
WS_1:\text{elongated block}
\end{cases}
@@

>Its hard to explain jokes to kleptomaniacs because they always *take things literally*.

In this [famous](https://en.wikipedia.org/wiki/Time_flies_like_an_arrow;_fruit_flies_like_a_banana) example:

>Time *flies like* an arrow, fruit *flies like* a banana*.

@@
flies
\begin{cases}
POS_l:\text{verb} \newline
POS_r:\text{noun}
\end{cases}
\space like
\begin{cases}
WS_l:\text{similarly to} \newline
WS_r:\text{love}
\end{cases}
@@

where *flies* is first a verb and *like* is used in the sense of analogy but then *flies* is a noun and *like* has the sense of prefer.

## Modelling uncertainty

Uncertainty of a word sense can be represented in serval forms. The most common is using probabilities but other options can be using [fuzzy logic](https://en.wikipedia.org/wiki/Fuzzy_logic) or even [Dempster Shafer theory](https://en.wikipedia.org/wiki/Dempster%E2%80%93Shafer_theory)


```python
word_sense={}
word_sense[('hit','struck' )] = 0.9 # mundane
word_sense[('hit','occured')] = 0.1 # inspired

# resolve word_sense based on the maximal likelihood estimate of the sequence
```



The probabilities here reflect the likelihood of encountering each word sense. The time is may take to get a joke may well be proportional to the ratio between probabilities of the of the mundane to the inspired.

## Modeling multiple meaning.

This is more subtle and also more powerful. In game theory we have simple strategies (embodying a single more) but there is also a mixed strategy which is a distribution over pure strategies.


```python

# if the a parse if good keep it around.
for prase,score in deep_parser(sentence)
   if score > threshold(sentence):
      contrafactual_distribution[parse]=score
```

with embeddings there is another option we use a weighed sum of the two senses.

```python
hit = hit_ws1 * .5 + hit_ws2 * .5
```

The issue is that even if we are keeping contrafactuals alive in parallel once we get to the final word and see it is 'ducks' our view of the world collapses from an equilibrium to a switch to @WS_2@. Perhaps the context switch is the.


Todos:

 - what is a good way to represent a finite probability distribution in python.
- find how to create & sample from a contingency table.

```python

   space = set()
   for coin in ['H','T']:
      for d6 in range(6):
         for day in range(7)
            space += (coin,d6,day)
   
```

